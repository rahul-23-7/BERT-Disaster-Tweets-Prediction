{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049; text-align: center; border-radius: 5px 5px; padding: 5px\">Train, Deploy and Update a HuggingFace BERT model on Disaster Tweets Classification dataset using AWS SageMaker </h3>\n",
    "\n",
    "NLP techniques such as [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), [word2vec](https://en.wikipedia.org/wiki/Word2vec), or [bag-of-words (BOW)](https://en.wikipedia.org/wiki/Bag-of-words_model) used to generate word embeddings features which can be used for training text classification models. They have been very successful in many NLP tasks but they don’t always capture the meanings of words accurately when they appear in different contexts.\n",
    "\n",
    "We achieved the better results in text classification tasks with the help of [BERT](https://arxiv.org/abs/1810.04805) because of its ability more accurately encode the meaning of words in different contexts.\n",
    "\n",
    "[Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/index.html) enables developers to create, train, deploy and monitor machine-learning models in the cloud.\n",
    "\n",
    "In this example, we walk through our dataset, the training process, and finally model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '0'></a>\n",
    "<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #007580; color : #fed049; border-radius: 5px 5px; text-align:center; font-weight: bold\" >Table of Contents</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Problem Statement](#1.0)\n",
    "2. [Setup](#2.0)\n",
    "3. [Data Preparation](#3.0)\n",
    "4. [EDA](#4.0)\n",
    "5. [Amazon SageMaker Training](#5.0)\n",
    "6. [Train on Amazon SageMaker using on-demand instances with Epoch=2](#6.0)\n",
    "7. [Train on Amazon SageMaker using spot instances](#7.0)\n",
    "8. [Host the model on an Amazon SageMaker Endpoint](#8.0)\n",
    "9. [Train on Amazon SageMaker using on-demand instances with Epoch=3](#9.0)\n",
    "10. [Update a SageMaker model endpoint](#10.0)\n",
    "11. [Clean up](#11.0)\n",
    "12. [Conclusion and references](#11.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '1.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 1. Problem Statement </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter (i.e. disaster relief organizations and news agencies). However, identifying such tweets has always been a difficult task because of the ambiguity in the linguistic structure of the tweets and hence it is not always clear whether an individual’s words are actually announcing a disaster.\n",
    "\n",
    "More details [here](https://www.kaggle.com/c/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 2. Setup </h2>\n",
    "\n",
    "To start, we import some Python libraries and initialize a SageMaker session, S3 bucket and prefix, and IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()    # Provides a collection of methods for working with SageMaker resources\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-huggingface-bert\"\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "role = sagemaker.get_execution_role()      # Get the execution role for the notebook instance. \n",
    "                                           # This is the IAM role that we created for our notebook instance. \n",
    "                                           # We pass the role to the tuning job(later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to supress display of warnings\n",
    "import warnings\n",
    "\n",
    "# suppress display of warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 3. Data Preparation </h2>\n",
    "\n",
    "Kaggle hosted a challenge named `Real` or `Not` whose aim was to use the Twitter data of disaster tweets, originally created by the company figure-eight, to classify Tweets talking about `real disaster` against the ones talking about it metaphorically. (https://www.kaggle.com/c/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `sentences` and `labels`\n",
    "\n",
    "Let us take a quick look at our data and for that we need to first read the training data. \n",
    "The only two columns we interested are:\n",
    "\n",
    "- the `sentence` (the tweet)\n",
    "- the `label`    (the label, this denotes whether a tweet is about a real disaster (1) or not (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"dataset/raw/data.csv\",\n",
    "    header=None,\n",
    "    usecols=[1, 3],\n",
    "    names=[\"label\", \"sentence\"],\n",
    ")\n",
    "\n",
    "\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  Our Deeds are the Reason of this #earthquake M...\n",
       "1      1             Forest fire near La Ronge Sask. Canada\n",
       "2      1  All residents asked to 'shelter in place' are ...\n",
       "3      1  13,000 people receive #wildfires evacuation or...\n",
       "4      1  Just got sent this photo from Ruby #Alaska as ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing few tweets with its class label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"mom: 'we didn't get home as fast as we wished' \\nme: 'why is that?'\\nmom: 'there was an accident and some truck spilt mayonnaise all over ??????\",\n",
       "  0),\n",
       " (\"I was in a horrible car accident this past Sunday. I'm finally able to get around. Thank you GOD??\",\n",
       "  1),\n",
       " ('Can wait to see how pissed Donnie is when I tell him I was in ANOTHER accident??',\n",
       "  0),\n",
       " (\"#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you've been in a crash&gt;http://t.co/Ld0unIYw4k\",\n",
       "  1),\n",
       " ('Accident in #Ashville on US 23 SB before SR 752 #traffic http://t.co/hylMo0WgFI',\n",
       "  1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(sentences[80:85], labels[80:85]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above output, there are few information which are not that important, like `URLs`, `Emojis`, `Tags`, etc. So, now lets try to clean the dataset before we actually pass this data for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to clean text by removing urls, emojis, html tags and punctuations.\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'] = df['sentence'].apply(lambda x: remove_URL(x))\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_emoji(x))\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_html(x))\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  Our Deeds are the Reason of this earthquake Ma...\n",
       "1      1              Forest fire near La Ronge Sask Canada\n",
       "2      1  All residents asked to shelter in place are be...\n",
       "3      1  13000 people receive wildfires evacuation orde...\n",
       "4      1  Just got sent this photo from Ruby Alaska as s..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mom we didnt get home as fast as we wished \\nme why is that\\nmom there was an accident and some truck spilt mayonnaise all over ',\n",
       "  0),\n",
       " ('I was in a horrible car accident this past Sunday Im finally able to get around Thank you GOD',\n",
       "  1),\n",
       " ('Can wait to see how pissed Donnie is when I tell him I was in ANOTHER accident',\n",
       "  0),\n",
       " ('TruckCrash Overturns On FortWorth Interstate  Click here if youve been in a crash',\n",
       "  1),\n",
       " ('Accident in Ashville on US 23 SB before SR 752 traffic ', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(sentences[80:85], labels[80:85]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 4. EDA </h2>\n",
    "\n",
    "Let's spend couple of minutes to explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Data Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x600 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\n",
    "sns.countplot(df['label'], ax=axes[0], palette=\"Set3\")\n",
    "axes[1].pie(df['label'].value_counts(),\n",
    "            labels=['Non Disaster', 'Real Disaster'],\n",
    "            autopct='%1.2f%%',\n",
    "            shadow=True,\n",
    "            explode=(0.05, 0),\n",
    "            startangle=60)\n",
    "fig.suptitle('Distribution of the Tweets', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Word Count'] = df['sentence'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "def plot_dist_char(df, feature, title):\n",
    "    # Creating a customized chart. and giving in figsize and everything.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 15))\n",
    "    # Creating a grid of 3 cols and 3 rows.\n",
    "    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "\n",
    "    # Customizing the histogram grid.\n",
    "    ax1 = fig.add_subplot(grid[0, :2])\n",
    "    # Set the title.\n",
    "    ax1.set_title('Histogram')\n",
    "    # plot the histogram.\n",
    "    sns.distplot(df.loc[:, feature],\n",
    "                 hist=True,\n",
    "                 kde=True,\n",
    "                 ax=ax1,\n",
    "                 color=\"b\",rug=True)\n",
    "    plt.axvline(df.loc[:, feature].mean(), linestyle=\"dashed\",color=\"k\", label='mean',linewidth=2)\n",
    "    ax1.set(ylabel='Frequency')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
    "\n",
    "    plt.title(f'{title}', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_dist_char(df[df['label'] == 0], 'Word Count', 'Words Per \"Non Disaster Tweet\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_dist_char(df[df['label'] == 1], 'Word Count', 'Words Per \"Real Disaster\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset for `training` and `testing`\n",
    "\n",
    "We then split the dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)               # Default split ratio 75/25, we can modify using \"test_size\"\n",
    "train.to_csv(\"dataset/train.csv\", index=False)\n",
    "test.to_csv(\"dataset/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload both to Amazon S3 for use later\n",
    "\n",
    "The SageMaker Python SDK provides a helpful function for uploading to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"dataset/train.csv\", bucket=bucket, key_prefix=prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"dataset/test.csv\", bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '5.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 5. Amazon SageMaker Training </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker\n",
    "\n",
    "- When running a training job, SageMaker reads input data from Amazon S3, uses that data to train a model. \n",
    "- Training data from S3 is made available to the Model Training instance container, which is pulled from Amazon Elastic Container Registry(`ECR`). \n",
    "- The training job persists model artifacts back to the output S3 location designated in the training job configuration. \n",
    "- When we are ready to deploy a model, SageMaker spins up new ML instances and pulls in these model artifacts to use for batch or real-time model inference.\n",
    "\n",
    "<img src = \"img/sm-training.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script\n",
    "\n",
    "We use the [PyTorch-Transformers library](https://pytorch.org/hub/huggingface_pytorch-transformers), which contains PyTorch implementations and pre-trained model weights for many NLP models, including BERT.\n",
    "\n",
    "Our training script should save model artifacts learned during training to a file path called `model_dir`, as stipulated by the SageMaker PyTorch image. Upon completion of training, model artifacts saved in `model_dir` will be uploaded to S3 by SageMaker and will become available in S3 for deployment.\n",
    "\n",
    "We save this script in a file named `train_deploy.py`, and put the file in a directory named `code/`. The full training script can be viewed under `code/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader, RandomSampler, TensorDataset\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AdamW, BertForSequenceClassification, BertTokenizer\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "MAX_LEN = \u001b[34m64\u001b[39;49;00m  \u001b[37m# this is the max length of the sentence\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading BERT tokenizer...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "tokenizer = BertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, do_lower_case=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mflat_accuracy\u001b[39;49;00m(preds, labels):\n",
      "    pred_flat = np.argmax(preds, axis=\u001b[34m1\u001b[39;49;00m).flatten()\n",
      "    labels_flat = labels.flatten()\n",
      "    \u001b[34mreturn\u001b[39;49;00m np.sum(pred_flat == labels_flat) / \u001b[36mlen\u001b[39;49;00m(labels_flat)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    sentences = dataset.sentence.values\n",
      "    labels = dataset.label.values\n",
      "\n",
      "    input_ids = []\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m sentences:\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        input_ids.append(encoded_sent)\n",
      "\n",
      "    \u001b[37m# pad shorter sentences\u001b[39;49;00m\n",
      "    input_ids_padded = []\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m input_ids:\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(i) < MAX_LEN:\n",
      "            i.append(\u001b[34m0\u001b[39;49;00m)\n",
      "        input_ids_padded.append(i)\n",
      "    input_ids = input_ids_padded\n",
      "\n",
      "    \u001b[37m# mask; 0: added, 1: otherwise\u001b[39;49;00m\n",
      "    attention_masks = []\n",
      "    \u001b[37m# For each sentence...\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m input_ids:\n",
      "        att_mask = [\u001b[36mint\u001b[39;49;00m(token_id > \u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token_id \u001b[35min\u001b[39;49;00m sent]\n",
      "        attention_masks.append(att_mask)\n",
      "\n",
      "    \u001b[37m# convert to PyTorch data types.\u001b[39;49;00m\n",
      "    train_inputs = torch.tensor(input_ids)\n",
      "    train_labels = torch.tensor(labels)\n",
      "    train_masks = torch.tensor(attention_masks)\n",
      "\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
      "    train_sampler = RandomSampler(train_data)\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataloader\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, training_dir):\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    sentences = dataset.sentence.values\n",
      "    labels = dataset.label.values\n",
      "\n",
      "    input_ids = []\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m sentences:\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        input_ids.append(encoded_sent)\n",
      "\n",
      "    \u001b[37m# pad shorter sentences\u001b[39;49;00m\n",
      "    input_ids_padded = []\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m input_ids:\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(i) < MAX_LEN:\n",
      "            i.append(\u001b[34m0\u001b[39;49;00m)\n",
      "        input_ids_padded.append(i)\n",
      "    input_ids = input_ids_padded\n",
      "\n",
      "    \u001b[37m# mask; 0: added, 1: otherwise\u001b[39;49;00m\n",
      "    attention_masks = []\n",
      "    \u001b[37m# For each sentence...\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m input_ids:\n",
      "        att_mask = [\u001b[36mint\u001b[39;49;00m(token_id > \u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token_id \u001b[35min\u001b[39;49;00m sent]\n",
      "        attention_masks.append(att_mask)\n",
      "\n",
      "    \u001b[37m# convert to PyTorch data types.\u001b[39;49;00m\n",
      "    train_inputs = torch.tensor(input_ids)\n",
      "    train_labels = torch.tensor(labels)\n",
      "    train_masks = torch.tensor(attention_masks)\n",
      "\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
      "    train_sampler = RandomSampler(train_data)\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataloader\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, args.num_gpus)\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir)\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.test)\n",
      "\n",
      "    logger.debug(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    logger.debug(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.sampler),\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting BertForSequenceClassification\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = BertForSequenceClassification.from_pretrained(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,  \u001b[37m# Use the 12-layer BERT model, with an uncased vocab.\u001b[39;49;00m\n",
      "        num_labels=args.num_labels,  \u001b[37m# The number of output labels--2 for binary classification.\u001b[39;49;00m\n",
      "        output_attentions=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns attentions weights.\u001b[39;49;00m\n",
      "        output_hidden_states=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns all hidden-states.\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    model = model.to(device)\n",
      "\n",
      "    \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\n",
      "    model = torch.nn.DataParallel(model)\n",
      "    optimizer = AdamW(\n",
      "        model.parameters(),\n",
      "        lr=\u001b[34m2e-5\u001b[39;49;00m,  \u001b[37m# args.learning_rate - default is 5e-5, our notebook had 2e-5\u001b[39;49;00m\n",
      "        eps=\u001b[34m1e-8\u001b[39;49;00m,  \u001b[37m# args.adam_epsilon - default is 1e-8.\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnd of defining BertForSequenceClassification\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        total_loss = \u001b[34m0\u001b[39;49;00m\n",
      "        model.train()\n",
      "        \u001b[34mfor\u001b[39;49;00m step, batch \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "            b_input_ids = batch[\u001b[34m0\u001b[39;49;00m].to(device)\n",
      "            b_input_mask = batch[\u001b[34m1\u001b[39;49;00m].to(device)\n",
      "            b_labels = batch[\u001b[34m2\u001b[39;49;00m].to(device)\n",
      "            model.zero_grad()\n",
      "\n",
      "            outputs = model(b_input_ids, token_type_ids=\u001b[34mNone\u001b[39;49;00m, attention_mask=b_input_mask, labels=b_labels)\n",
      "            loss = outputs[\u001b[34m0\u001b[39;49;00m]\n",
      "\n",
      "            total_loss += loss.item()\n",
      "            loss.backward()\n",
      "            torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[34m1.0\u001b[39;49;00m)\n",
      "            \u001b[37m# modified based on their gradients, the learning rate, etc.\u001b[39;49;00m\n",
      "            optimizer.step()\n",
      "            \u001b[34mif\u001b[39;49;00m step % args.log_interval == \u001b[34m0\u001b[39;49;00m:\n",
      "                logger.info(\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "                        epoch,\n",
      "                        step * \u001b[36mlen\u001b[39;49;00m(batch[\u001b[34m0\u001b[39;49;00m]),\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * step / \u001b[36mlen\u001b[39;49;00m(train_loader),\n",
      "                        loss.item(),\n",
      "                    )\n",
      "                )\n",
      "\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mAverage training loss: \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader))\n",
      "\n",
      "        test(model, test_loader, device)\n",
      "\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving tuned model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model_2_save = model.module \u001b[34mif\u001b[39;49;00m \u001b[36mhasattr\u001b[39;49;00m(model, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodule\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34melse\u001b[39;49;00m model\n",
      "    model_2_save.save_pretrained(save_directory=args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\n",
      "    model.eval()\n",
      "    _, eval_accuracy = \u001b[34m0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            b_input_ids = batch[\u001b[34m0\u001b[39;49;00m].to(device)\n",
      "            b_input_mask = batch[\u001b[34m1\u001b[39;49;00m].to(device)\n",
      "            b_labels = batch[\u001b[34m2\u001b[39;49;00m].to(device)\n",
      "\n",
      "            outputs = model(b_input_ids, token_type_ids=\u001b[34mNone\u001b[39;49;00m, attention_mask=b_input_mask)\n",
      "            logits = outputs[\u001b[34m0\u001b[39;49;00m]\n",
      "            logits = logits.detach().cpu().numpy()\n",
      "            label_ids = b_labels.to(\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).numpy()\n",
      "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
      "            eval_accuracy += tmp_eval_accuracy\n",
      "\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest set: Accuracy: \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, tmp_eval_accuracy)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ objects in model_dir ===================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(model_dir))\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_path:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, model_path)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(model_path))\n",
      "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ model loaded ===========================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\n",
      "    \u001b[33m\"\"\"An input_fn that loads a pickled tensor\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m request_content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        data = json.loads(request_body)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ input sentences ===============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(data)\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mstr\u001b[39;49;00m):\n",
      "            data = [data]\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mlist\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(data) > \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data[\u001b[34m0\u001b[39;49;00m], \u001b[36mstr\u001b[39;49;00m):\n",
      "            \u001b[34mpass\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported input type. Input type can be a string or an non-empty list. \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m                             I got \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data))\n",
      "                       \n",
      "        input_ids = [tokenizer.encode(x, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m data]\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ encoded sentences ==============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(input_ids)\n",
      "\n",
      "        \u001b[37m# pad shorter sentence\u001b[39;49;00m\n",
      "        padded =  torch.zeros(\u001b[36mlen\u001b[39;49;00m(input_ids), MAX_LEN) \n",
      "        \u001b[34mfor\u001b[39;49;00m i, p \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(input_ids):\n",
      "            padded[i, :\u001b[36mlen\u001b[39;49;00m(p)] = torch.tensor(p)\n",
      "     \n",
      "        \u001b[37m# create mask\u001b[39;49;00m\n",
      "        mask = (padded != \u001b[34m0\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================= padded input and attention mask ================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(padded, \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, mask)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m padded.long(), mask.long()\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported content type: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(request_content_type))\n",
      "    \n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model.to(device)\n",
      "    model.eval()\n",
      "\n",
      "    input_id, input_mask = input_data\n",
      "    input_id = input_id.to(device)\n",
      "    input_mask = input_mask.to(device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m============== encoded data =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(input_id, input_mask)\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        y = model(input_id, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=============== inference result =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(y)\n",
      "    \u001b[34mreturn\u001b[39;49;00m y\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--num_labels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m50\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.path.join(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m'\u001b[39;49;00m\u001b[33mmodel/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TESTING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    train(parser.parse_args())\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train_deploy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '6.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 6. Train on Amazon SageMaker using on-demand instances with Epoch=2</h2>\n",
    "\n",
    "We use Amazon SageMaker to train and deploy a model using our custom PyTorch code. The Amazon SageMaker Python SDK makes it easier to run a PyTorch script in Amazon SageMaker using its PyTorch estimator. After that, we can use the SageMaker Python SDK to deploy the trained model and run predictions. For more information on how to use this SDK with PyTorch, see [the SageMaker Python SDK documentation](https://sagemaker.readthedocs.io/en/stable/using_pytorch.html).\n",
    "\n",
    "To start, we use the `PyTorch` estimator class to train our model. When creating our estimator, we make sure to specify a few things:\n",
    "\n",
    "* `entry_point`: the name of our PyTorch script. It contains our training script, which loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model. It also contains code to load and run the model during inference.\n",
    "* `source_dir`: the location of our training scripts and requirements.txt file. \"requirements.txt\" lists packages you want to use with your script.\n",
    "* `framework_version`: the PyTorch version we want to use\n",
    "\n",
    "After creating the estimator, we then call fit(), which launches a training job. We use the Amazon S3 URIs where we uploaded the training data earlier.\n",
    "\n",
    "<img src = \"img/sm-estimator.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-14 09:13:19 Starting - Starting the training job...\n",
      "2022-05-14 09:13:42 Starting - Preparing the instances for trainingProfilerReport-1652519599: InProgress\n",
      "......\n",
      "2022-05-14 09:14:42 Downloading - Downloading input data...\n",
      "2022-05-14 09:15:02 Training - Downloading the training image...\n",
      "2022-05-14 09:15:50 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-14 09:15:52,084 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-14 09:15:52,086 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 09:15:52,097 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-14 09:15:52,106 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-14 09:15:52,498 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mCollecting requests==2.22.0\u001b[0m\n",
      "\u001b[34mDownloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.15.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting chardet<3.1.0,>=3.0.2\u001b[0m\n",
      "\u001b[34mDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mCollecting idna<2.9,>=2.5\u001b[0m\n",
      "\u001b[34mDownloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (1.21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.7.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 6)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 6)) (3.0.6)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=63b6495287c8cfbdaf107b8206dd0580950d5caa4dc78ec02a23e70611c6f98b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: urllib3, idna, chardet, requests, regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.7\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.7:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.7\u001b[0m\n",
      "\u001b[34mAttempting uninstall: idna\u001b[0m\n",
      "\u001b[34mFound existing installation: idna 3.3\u001b[0m\n",
      "\u001b[34mUninstalling idna-3.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled idna-3.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[34mFound existing installation: requests 2.26.0\u001b[0m\n",
      "\u001b[34mUninstalling requests-2.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled requests-2.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed chardet-3.0.4 filelock-3.7.0 huggingface-hub-0.6.0 idna-2.8 regex-2022.4.24 requests-2.22.0 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.15.0 urllib3-1.25.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-05-14 09:16:04,886 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 09:16:04,902 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 09:16:04,915 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 09:16:04,923 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-05-14-09-13-19-240\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-09-13-19-240/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-09-13-19-240/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-05-14-09-13-19-240\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-09-13-19-240/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_deploy.py --epochs 2 --num_labels 2\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 5.32MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 22.6kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 10.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 570/570 [00:00<00:00, 406kB/s]\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mProcesses 5709/5709 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34mStarting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/420M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 961k/420M [00:00<00:44, 9.84MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 6.64M/420M [00:00<00:11, 39.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 13.6M/420M [00:00<00:07, 54.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 20.9M/420M [00:00<00:06, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 29.6M/420M [00:00<00:05, 73.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 37.3M/420M [00:00<00:05, 75.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 44.5M/420M [00:00<00:05, 75.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 51.7M/420M [00:00<00:05, 75.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 59.6M/420M [00:00<00:04, 77.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 67.1M/420M [00:01<00:04, 76.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 74.3M/420M [00:01<00:04, 75.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 81.5M/420M [00:01<00:04, 71.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██▏       | 89.5M/420M [00:01<00:04, 75.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 97.6M/420M [00:01<00:04, 78.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 105M/420M [00:01<00:04, 78.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 114M/420M [00:01<00:03, 81.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 122M/420M [00:01<00:04, 75.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 129M/420M [00:01<00:04, 70.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 137M/420M [00:01<00:03, 75.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 145M/420M [00:02<00:03, 75.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 152M/420M [00:02<00:03, 71.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 160M/420M [00:02<00:03, 75.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 168M/420M [00:02<00:03, 77.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 176M/420M [00:02<00:03, 74.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▎     | 183M/420M [00:02<00:03, 73.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 191M/420M [00:02<00:03, 78.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 200M/420M [00:02<00:02, 81.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 209M/420M [00:02<00:02, 84.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 217M/420M [00:03<00:02, 83.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 225M/420M [00:03<00:02, 80.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 232M/420M [00:03<00:02, 77.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 240M/420M [00:03<00:02, 78.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 248M/420M [00:03<00:02, 77.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 255M/420M [00:03<00:02, 76.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 263M/420M [00:03<00:02, 76.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 270M/420M [00:03<00:02, 69.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 277M/420M [00:03<00:02, 70.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 284M/420M [00:04<00:01, 72.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 292M/420M [00:04<00:01, 75.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████▏  | 299M/420M [00:04<00:01, 73.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 307M/420M [00:04<00:01, 74.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 315M/420M [00:04<00:01, 76.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 322M/420M [00:04<00:01, 78.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 330M/420M [00:04<00:01, 78.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 337M/420M [00:04<00:01, 76.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 345M/420M [00:04<00:01, 73.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 352M/420M [00:04<00:00, 73.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 359M/420M [00:05<00:00, 74.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 366M/420M [00:05<00:00, 74.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 374M/420M [00:05<00:00, 75.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 381M/420M [00:05<00:00, 74.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 388M/420M [00:05<00:00, 70.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 396M/420M [00:05<00:00, 73.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 403M/420M [00:05<00:00, 74.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 410M/420M [00:05<00:00, 73.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 417M/420M [00:05<00:00, 74.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 420M/420M [00:05<00:00, 74.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mEnd of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.314 algo-1:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.576 algo-1:42 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.576 algo-1:42 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.577 algo-1:42 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.577 algo-1:42 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.577 algo-1:42 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.590 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.591 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.592 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.593 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.594 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.595 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.596 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.597 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.598 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.599 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:593] Total Trainable Params: 109483778\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.600 algo-1:42 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-05-14 09:16:20.603 algo-1:42 INFO hook.py:488] Hook is writing from the hook with pid: 42\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/5709 (0%)] Loss: 0.752307\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/5709 (0%)] Loss: 0.752307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/5709 (56%)] Loss: 0.518363\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/5709 (56%)] Loss: 0.518363\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.473186\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.473186\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.825221\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.825221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [0/5709 (0%)] Loss: 0.367052\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [0/5709 (0%)] Loss: 0.367052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3200/5709 (56%)] Loss: 0.313773\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3200/5709 (56%)] Loss: 0.313773\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.356163\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.356163\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.824115\u001b[0m\n",
      "\u001b[34mSaving tuned model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.824115\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m2022-05-14 10:15:36,540 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-05-14 10:16:16 Uploading - Uploading generated training model\n",
      "2022-05-14 10:17:16 Completed - Training job completed\n",
      "ProfilerReport-1652519599: NoIssuesFound\n",
      "Training seconds: 3755\n",
      "Billable seconds: 3755\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# 1. Defining the estimator \n",
    "\n",
    "estimator = PyTorch(entry_point=\"train_deploy.py\",\n",
    "                    source_dir=\"code\",\n",
    "                    role=role,\n",
    "                    framework_version=\"1.9\",\n",
    "                    py_version=\"py38\",\n",
    "                    instance_count=1,\n",
    "                    instance_type=\"ml.m5.xlarge\",             # Type of instance we want the training to happen\n",
    "                    hyperparameters={\"epochs\": 2,\n",
    "                                     \"num_labels\": 2,\n",
    "                                    }\n",
    "                   )\n",
    "\n",
    "# 2. Start the Training \n",
    "\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id = '7.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 7. Train on Amazon SageMaker using spot instances </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-14 12:15:48 Starting - Starting the training job...\n",
      "2022-05-14 12:15:49 Starting - Launching requested ML instancesProfilerReport-1652530547: InProgress\n",
      "......\n",
      "2022-05-14 12:17:14 Starting - Preparing the instances for training.........\n",
      "2022-05-14 12:18:42 Downloading - Downloading input data...\n",
      "2022-05-14 12:19:08 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:08,924 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:08,927 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:08,938 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:15,192 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:15,629 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mCollecting requests==2.22.0\u001b[0m\n",
      "\u001b[34mDownloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.15.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\u001b[0m\n",
      "\u001b[34mCollecting idna<2.9,>=2.5\u001b[0m\n",
      "\u001b[34mDownloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mCollecting chardet<3.1.0,>=3.0.2\u001b[0m\n",
      "\u001b[34mDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (1.21.2)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.7.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 6)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 6)) (3.0.6)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e3f5237f62e46666ae62eec037c495cd699da67cc3e9ae72f1fa5bd37ddbefba\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: urllib3, idna, chardet, requests, regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.7\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.7:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.7\u001b[0m\n",
      "\u001b[34mAttempting uninstall: idna\u001b[0m\n",
      "\u001b[34mFound existing installation: idna 3.3\u001b[0m\n",
      "\u001b[34mUninstalling idna-3.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled idna-3.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[34mFound existing installation: requests 2.26.0\u001b[0m\n",
      "\u001b[34mUninstalling requests-2.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled requests-2.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed chardet-3.0.4 filelock-3.7.0 huggingface-hub-0.6.0 idna-2.8 regex-2022.4.24 requests-2.22.0 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.15.0 urllib3-1.25.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:26,994 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:27,005 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:27,015 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 12:19:27,024 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-05-14-12-15-47-723\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-12-15-47-723/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-12-15-47-723/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-05-14-12-15-47-723\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-12-15-47-723/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_deploy.py --epochs 2 --num_labels 2\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 5.04MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 27.0kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 7.93MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 570/570 [00:00<00:00, 542kB/s]\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mProcesses 5709/5709 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34mStarting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/420M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 896k/420M [00:00<00:48, 9.13MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 7.43M/420M [00:00<00:09, 44.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 13.7M/420M [00:00<00:07, 54.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 21.9M/420M [00:00<00:06, 66.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 29.9M/420M [00:00<00:05, 72.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 37.9M/420M [00:00<00:05, 76.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 45.7M/420M [00:00<00:05, 78.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 54.3M/420M [00:00<00:04, 82.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 62.5M/420M [00:00<00:04, 83.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 70.7M/420M [00:01<00:04, 84.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 79.1M/420M [00:01<00:04, 85.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 87.5M/420M [00:01<00:04, 86.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 95.8M/420M [00:01<00:03, 86.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 104M/420M [00:01<00:03, 85.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 112M/420M [00:01<00:03, 85.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▊       | 121M/420M [00:01<00:03, 86.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 129M/420M [00:01<00:03, 85.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 137M/420M [00:01<00:03, 85.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 145M/420M [00:01<00:03, 86.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 154M/420M [00:02<00:03, 86.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▊      | 162M/420M [00:02<00:03, 87.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 171M/420M [00:02<00:02, 87.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 179M/420M [00:02<00:02, 87.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 187M/420M [00:02<00:02, 86.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 196M/420M [00:02<00:02, 86.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▊     | 204M/420M [00:02<00:02, 86.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 212M/420M [00:02<00:02, 86.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 221M/420M [00:02<00:02, 86.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 229M/420M [00:02<00:02, 86.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▋    | 237M/420M [00:03<00:02, 86.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 246M/420M [00:03<00:02, 86.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 254M/420M [00:03<00:01, 87.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 263M/420M [00:03<00:01, 87.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 271M/420M [00:03<00:01, 87.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▋   | 279M/420M [00:03<00:01, 87.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 288M/420M [00:03<00:01, 87.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 296M/420M [00:03<00:01, 86.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 304M/420M [00:03<00:01, 86.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 313M/420M [00:03<00:01, 87.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▋  | 321M/420M [00:04<00:01, 87.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 330M/420M [00:04<00:01, 88.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 338M/420M [00:04<00:00, 88.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 347M/420M [00:04<00:00, 89.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 356M/420M [00:04<00:00, 90.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 365M/420M [00:04<00:00, 90.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 373M/420M [00:04<00:00, 90.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 382M/420M [00:04<00:00, 89.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 391M/420M [00:04<00:00, 90.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 399M/420M [00:04<00:00, 90.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 408M/420M [00:05<00:00, 90.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 417M/420M [00:05<00:00, 90.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 420M/420M [00:05<00:00, 85.2MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mEnd of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.728 algo-1:41 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.973 algo-1:41 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.973 algo-1:41 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.973 algo-1:41 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.974 algo-1:41 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.974 algo-1:41 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.986 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.986 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.986 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.986 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.987 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.988 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.989 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.990 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.991 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.992 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.993 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.994 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:593] Total Trainable Params: 109483778\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.995 algo-1:41 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-05-14 12:19:40.998 algo-1:41 INFO hook.py:488] Hook is writing from the hook with pid: 41\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/5709 (0%)] Loss: 0.752307\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/5709 (0%)] Loss: 0.752307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/5709 (56%)] Loss: 0.518363\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/5709 (56%)] Loss: 0.518363\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.473186\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.473186\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.825221\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.825221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [0/5709 (0%)] Loss: 0.367052\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [0/5709 (0%)] Loss: 0.367052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3200/5709 (56%)] Loss: 0.313773\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3200/5709 (56%)] Loss: 0.313773\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.356163\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.356163\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.824115\u001b[0m\n",
      "\u001b[34mSaving tuned model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.824115\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m2022-05-14 12:55:39,669 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-05-14 12:55:53 Uploading - Uploading generated training model\n",
      "2022-05-14 12:56:54 Completed - Training job completed\n",
      "ProfilerReport-1652530547: NoIssuesFound\n",
      "Training seconds: 2282\n",
      "Billable seconds: 508\n",
      "Managed Spot Training savings: 77.7%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# 1. Defining the estimator \n",
    "\n",
    "estimator = PyTorch(entry_point=\"train_deploy.py\",\n",
    "                    source_dir=\"code\",\n",
    "                    role=role,\n",
    "                    framework_version=\"1.9\",\n",
    "                    py_version=\"py38\",\n",
    "                    instance_count=1,\n",
    "                    instance_type=\"ml.m5.xlarge\",             # Type of instance we want the training to happen\n",
    "                    hyperparameters={\"epochs\": 2,\n",
    "                                     \"num_labels\": 2,\n",
    "                                    },\n",
    "                    use_spot_instances=True,\n",
    "                    max_run=4000,\n",
    "                    max_wait=5000\n",
    "                   )\n",
    "\n",
    "# 2. Start the Training \n",
    "\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '8.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 8. Host the model on an Amazon SageMaker Endpoint </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we host it on an Amazon SageMaker Endpoint. To make the endpoint load the model and serve predictions, we implement a few methods in `train_deploy.py`.\n",
    "\n",
    "* `model_fn()`: function defined to load the saved model and return a model object that can be used for model serving. The SageMaker PyTorch model server loads our model by invoking model_fn.\n",
    "* `input_fn()`: deserializes and prepares the prediction input. In this example, our request body is first serialized to JSON and then sent to model serving endpoint. Therefore, in `input_fn()`, we first deserialize the JSON-formatted request body and return the input as a `torch.tensor`, as required for BERT.\n",
    "* `predict_fn()`: performs the prediction and returns the result.\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our PyTorch estimator object, passing in our desired number of instances and instance type:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure the predictor to use `application/json` for the content type when sending requests to our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction/Inferance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the returned predictor object to call the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = {1: \"Real disaster\",\n",
    "               0: \"Not a disaster\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"I met my friend today by accident\",\n",
    "                  \"Frank had a severe head injury after the car accident last month\", \n",
    "                  \"Just happened a terrible car crash\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(test_sentences)\n",
    "result = list(np.argmax(result, axis=1))\n",
    "\n",
    "predicted_labels = [class_label[l] for l in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not a disaster', 'Real disaster', 'Real disaster']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I met my friend today by accident ---> Not a disaster\n",
      "Frank had a severe head injury after the car accident last month ---> Real disaster\n",
      "Just happened a terrible car crash ---> Real disaster\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in zip(test_sentences, predicted_labels):\n",
    "    print(f\"{tweet} ---> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '9.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 9. Train on Amazon SageMaker using on-demand instances with Epoch=3 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-14 10:28:20 Starting - Starting the training job...\n",
      "2022-05-14 10:28:46 Starting - Preparing the instances for trainingProfilerReport-1652524100: InProgress\n",
      "......\n",
      "2022-05-14 10:29:46 Downloading - Downloading input data...\n",
      "2022-05-14 10:30:06 Training - Downloading the training image.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-14 10:30:57,266 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-14 10:30:57,268 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 10:30:57,277 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-14 10:30:57,283 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-14 10:30:57,638 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mCollecting requests==2.22.0\u001b[0m\n",
      "\u001b[34mDownloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.15.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting chardet<3.1.0,>=3.0.2\u001b[0m\n",
      "\u001b[34mDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\u001b[0m\n",
      "\u001b[34mCollecting idna<2.9,>=2.5\u001b[0m\n",
      "\u001b[34mDownloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.7.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.15.0->-r requirements.txt (line 6)) (1.21.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 6)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 6)) (3.0.6)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=d7a5867c0fda0770c2396e8d20473eda934526c6d5e9ccd3186882fb53bfeaa4\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: urllib3, idna, chardet, requests, regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.7\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.7:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.7\u001b[0m\n",
      "\u001b[34mAttempting uninstall: idna\u001b[0m\n",
      "\u001b[34mFound existing installation: idna 3.3\u001b[0m\n",
      "\u001b[34mUninstalling idna-3.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled idna-3.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[34mFound existing installation: requests 2.26.0\u001b[0m\n",
      "\u001b[34mUninstalling requests-2.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled requests-2.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed chardet-3.0.4 filelock-3.7.0 huggingface-hub-0.6.0 idna-2.8 regex-2022.4.24 requests-2.22.0 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.15.0 urllib3-1.25.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\n",
      "2022-05-14 10:31:06 Training - Training image download completed. Training in progress.\u001b[34m2022-05-14 10:31:09,477 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 10:31:09,493 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 10:31:09,506 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-14 10:31:09,515 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 3,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-05-14-10-28-20-287\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-10-28-20-287/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-10-28-20-287/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-05-14-10-28-20-287\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-10-28-20-287/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_deploy.py --epochs 3 --num_labels 2\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 5.11MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 38.5kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 7.71MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 570/570 [00:00<00:00, 701kB/s]\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mProcesses 5709/5709 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 1904/1904 (100%) of test data\u001b[0m\n",
      "\u001b[34mStarting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/420M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 860k/420M [00:00<00:50, 8.79MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|▏         | 5.81M/420M [00:00<00:12, 34.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 13.4M/420M [00:00<00:07, 54.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 20.5M/420M [00:00<00:06, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 27.0M/420M [00:00<00:06, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 35.4M/420M [00:00<00:05, 72.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 43.4M/420M [00:00<00:05, 76.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 50.6M/420M [00:00<00:05, 73.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▎        | 57.7M/420M [00:00<00:05, 68.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 64.5M/420M [00:01<00:05, 69.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 71.5M/420M [00:01<00:05, 70.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▊        | 78.3M/420M [00:01<00:05, 68.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 86.6M/420M [00:01<00:04, 73.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 94.1M/420M [00:01<00:04, 75.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 101M/420M [00:01<00:04, 74.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 109M/420M [00:01<00:04, 77.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 118M/420M [00:01<00:03, 81.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 126M/420M [00:01<00:03, 79.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 133M/420M [00:01<00:03, 80.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 141M/420M [00:02<00:03, 79.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 149M/420M [00:02<00:03, 76.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 157M/420M [00:02<00:03, 80.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 166M/420M [00:02<00:03, 84.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 176M/420M [00:02<00:02, 89.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 186M/420M [00:02<00:02, 92.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 195M/420M [00:02<00:02, 95.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 205M/420M [00:02<00:02, 97.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 214M/420M [00:02<00:02, 93.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 223M/420M [00:03<00:02, 87.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 231M/420M [00:03<00:02, 86.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 241M/420M [00:03<00:02, 89.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 249M/420M [00:03<00:01, 90.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████▏   | 258M/420M [00:03<00:01, 89.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▎   | 267M/420M [00:03<00:01, 91.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 276M/420M [00:03<00:01, 92.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 285M/420M [00:03<00:01, 86.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 294M/420M [00:03<00:01, 88.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 302M/420M [00:03<00:01, 86.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 311M/420M [00:04<00:01, 88.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 320M/420M [00:04<00:01, 89.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 329M/420M [00:04<00:01, 89.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 337M/420M [00:04<00:01, 85.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 345M/420M [00:04<00:00, 83.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 354M/420M [00:04<00:00, 86.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 364M/420M [00:04<00:00, 89.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▊ | 372M/420M [00:04<00:00, 84.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 381M/420M [00:04<00:00, 86.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 390M/420M [00:05<00:00, 90.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 399M/420M [00:05<00:00, 88.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 408M/420M [00:05<00:00, 89.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 418M/420M [00:05<00:00, 93.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 420M/420M [00:05<00:00, 82.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mEnd of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.273 algo-1:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.505 algo-1:42 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.505 algo-1:42 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.506 algo-1:42 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.506 algo-1:42 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.506 algo-1:42 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.517 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.518 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.519 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.520 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.521 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.522 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.523 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.524 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.525 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.526 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:593] Total Trainable Params: 109483778\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.527 algo-1:42 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-05-14 10:31:24.530 algo-1:42 INFO hook.py:488] Hook is writing from the hook with pid: 42\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/5709 (0%)] Loss: 0.752307\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/5709 (0%)] Loss: 0.752307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/5709 (56%)] Loss: 0.518363\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/5709 (56%)] Loss: 0.518363\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.473186\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.473186\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.825221\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.825221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [0/5709 (0%)] Loss: 0.367052\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [0/5709 (0%)] Loss: 0.367052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3200/5709 (56%)] Loss: 0.313773\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [3200/5709 (56%)] Loss: 0.313773\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.356163\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.356163\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.824115\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.824115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [0/5709 (0%)] Loss: 0.345569\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [0/5709 (0%)] Loss: 0.345569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [3200/5709 (56%)] Loss: 0.371484\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [3200/5709 (56%)] Loss: 0.371484\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.274791\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.274791\u001b[0m\n",
      "\n",
      "2022-05-14 11:59:06 Uploading - Uploading generated training model\u001b[34mTest set: Accuracy: 0.826327\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.826327\u001b[0m\n",
      "\u001b[34mSaving tuned model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m2022-05-14 11:59:03,716 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-05-14 12:00:12 Completed - Training job completed\n",
      "Training seconds: 5438\n",
      "Billable seconds: 5438\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# 1. Defining the estimator \n",
    "\n",
    "estimator = PyTorch(entry_point=\"train_deploy.py\",\n",
    "                    source_dir=\"code\",\n",
    "                    role=role,\n",
    "                    framework_version=\"1.9\",\n",
    "                    py_version=\"py38\",\n",
    "                    instance_count=1,                         \n",
    "                    instance_type=\"ml.m5.xlarge\",             # Type of instance we want the training to happen\n",
    "                    hyperparameters={\"epochs\": 3,\n",
    "                                     \"num_labels\": 2\n",
    "                                    }\n",
    "                   )\n",
    "\n",
    "# 2. Start the Training \n",
    "\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '10.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 10. Update a SageMaker model endpoint </h2>\n",
    "\n",
    "As we know Machine Learning is a highly iterative process. During the course of a single project, data scientists and ML engineers routinely train thousands of different models in search of maximum accuracy. Indeed, the number of combinations for algorithms, data sets, and training parameters (aka hyperparameters) is infinite.\n",
    "\n",
    "For example, let’s train a BERT model with updated hyperparameter (epoch = 3) value and see if model performance is improved or not.\n",
    "\n",
    "If we find the improvement in model performance then we can update the existing SageMaker model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/'\n",
    "code_path = 'code/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-10-28-20-287/output/model.tar.gz to model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {estimator.model_data} {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/\n",
      "model/config.json\n",
      "model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf {model_path}/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {model_path}/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/\n",
      "model/config.json\n",
      "model/pytorch_model.bin\n",
      "tar: model: file changed as we read it\n",
      "code/\n",
      "code/.ipynb_checkpoints/\n",
      "code/.ipynb_checkpoints/train_deploy-checkpoint.py\n",
      "code/train_deploy.py\n",
      "code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz {model_path}/model.tar.gz {model_path} {code_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: model/model.tar.gz to s3://sagemaker-us-east-2-920084877200/pytorch-training-2022-05-14-10-28-20-287/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {model_path}/model.tar.gz {estimator.model_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    py_version=\"py38\",\n",
    "    image_scope=\"inference\",\n",
    "    version=\"1.9\",\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    ")\n",
    "\n",
    "primary_container = {\n",
    "    'Image': image_uri,\n",
    "    'ModelDataUrl': estimator.model_data,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM': 'train_deploy.py',\n",
    "        'SAGEMAKER_REGION': region,\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY': estimator.model_data\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytorch-training-2022-05-14-10-28-20-287'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = estimator.model_data.split('/')[-3]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "   ModelName = model_name, ExecutionRoleArn = role, PrimaryContainer = primary_container\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "predictor.update_endpoint(initial_instance_count=1, instance_type=\"ml.m4.xlarge\", model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction/Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = {1: \"Real disaster\",\n",
    "               0: \"Not a disaster\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"I met my friend today by accident\",\n",
    "                  \"Frank had a severe head injury after the car accident last month\", \n",
    "                  \"Just happened a terrible car crash\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(test_sentences)\n",
    "result = list(np.argmax(result, axis=1))\n",
    "\n",
    "predicted_labels = [class_label[l] for l in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not a disaster', 'Real disaster', 'Real disaster']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I met my friend today by accident ---> Not a disaster\n",
      "Frank had a severe head injury after the car accident last month ---> Real disaster\n",
      "Just happened a terrible car crash ---> Real disaster\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in zip(test_sentences, predicted_labels):\n",
    "    print(f\"{tweet} ---> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's delete the Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '11.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 11. Clean up </h2>\n",
    "\n",
    "Lastly, please remember to delete the Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '12.0'></a>\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 12. Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hopefully this post will help you leverage the power of Amazon SageMaker to train, deploy and update a HuggingFace BERT models on your own data using Amazon in-built Deep learning containers.\n",
    "\n",
    "- Amazon SageMaker abstracts away the complexities related to maintaining secure and expensive GPU-powered virtual machines for training phase and also simplifies the process of deploying the model to production.\n",
    "\n",
    "- We went through Amazon SageMaker training process, understood how we can train a model using on-demand instances and spot instances.\n",
    "Model training using spot instances can save the cost of training models up to 90% over on-demand instances.\n",
    "\n",
    "- As we know Machine Learning is a highly iterative process. How one can update the hyperparameters and update a SageMaker model endpoint when there is an improvement on model performance.\n",
    "\n",
    "- Finally we have to clean up SageMaker endpoint to avoid charges.\n",
    "\n",
    "I would love to hear your suggestions on further improvements and also welcome your code contribution to the [github repo](https://github.com/Vinayaks117/AWS-SageMaker-Examples.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> References </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction to Transformers and BERT on Amazon SageMaker](https://www.youtube.com/watch?v=D9Qo5OpG4p8)\n",
    "- [Use PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "- [Prebuilt SageMaker Docker Images for Deep Learning](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html)\n",
    "- [Use Checkpoints in Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)\n",
    "- [Managed Spot Training in Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
